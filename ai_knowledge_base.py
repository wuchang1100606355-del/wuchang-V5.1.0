"""
AI çŸ¥è­˜åº«ç®¡ç†ç³»çµ±
ç”¨æ–¼åŠ è¼‰ã€å­¸ç¿’å’Œè¨˜æ†¶ç¤¾å€åˆ†æžæ•¸æ“š
"""

import json
import os
from datetime import datetime
from typing import Dict, Any, List, Optional
from pathlib import Path
import re

class AIKnowledgeBase:
    """AI çŸ¥è­˜åº«ç®¡ç†å™¨ - ç”¨æ–¼å­¸ç¿’å’Œè¨˜æ†¶ç¤¾å€æ•¸æ“š"""
    
    def __init__(self, knowledge_base_path='wuchang_community_knowledge_base.json'):
        """
        åˆå§‹åŒ–çŸ¥è­˜åº«
        
        Args:
            knowledge_base_path: çŸ¥è­˜åº« JSON æ–‡ä»¶è·¯å¾‘
        """
        self.knowledge_base_path = knowledge_base_path
        self.knowledge_data = {}
        self.learning_history = []
        self.memory_cache = {}
        self.index_path = 'wuchang_community_knowledge_index.json'
        self.index_data: Dict[str, Any] = {}
        self.load_knowledge_base()
        self.load_index()
    
    def load_knowledge_base(self):
        """åŠ è¼‰çŸ¥è­˜åº«æ•¸æ“š"""
        try:
            if os.path.exists(self.knowledge_base_path):
                with open(self.knowledge_base_path, 'r', encoding='utf-8') as f:
                    self.knowledge_data = json.load(f)
                # Avoid unicode output issues on some Windows consoles
                print(f"[OK] knowledge base loaded: {self.knowledge_base_path}")
                return True
            else:
                print(f"[WARN] knowledge base file not found: {self.knowledge_base_path}")
                return False
        except Exception as e:
            print(f"[ERROR] failed to load knowledge base: {e}")
            return False

    def load_index(self) -> bool:
        """åŠ è¼‰ï¼ˆå¯é¸ï¼‰ç´¢å¼•æª”ï¼Œç”¨æ–¼æ›´å¿«çš„æª¢ç´¢ã€‚"""
        try:
            if os.path.exists(self.index_path):
                with open(self.index_path, 'r', encoding='utf-8') as f:
                    self.index_data = json.load(f)
                print(f"[OK] knowledge index loaded: {self.index_path}")
                return True
            self.index_data = {}
            return False
        except Exception as e:
            print(f"[WARN] failed to load knowledge index: {e}")
            self.index_data = {}
            return False
    
    def learn_and_memorize(self, topic: str, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        å­¸ç¿’ä¸¦è¨˜æ†¶æ•¸æ“š
        
        Args:
            topic: ä¸»é¡Œ/é¡žåˆ¥
            data: è¦å­¸ç¿’çš„æ•¸æ“š
            
        Returns:
            dict: å­¸ç¿’çµæžœ
        """
        try:
            timestamp = datetime.now().isoformat()
            
            # è¨˜éŒ„å­¸ç¿’æ­·å²
            learning_record = {
                'topic': topic,
                'timestamp': timestamp,
                'data_summary': self._summarize_data(data),
                'data_size': len(str(data))
            }
            self.learning_history.append(learning_record)
            
            # å­˜å„²åˆ°è¨˜æ†¶ç·©å­˜
            if topic not in self.memory_cache:
                self.memory_cache[topic] = []
            
            self.memory_cache[topic].append({
                'data': data,
                'learned_at': timestamp,
                'access_count': 0
            })
            
            # ä¿æŒæœ€è¿‘ 1000 æ¢è¨˜æ†¶
            if len(self.memory_cache[topic]) > 1000:
                self.memory_cache[topic] = self.memory_cache[topic][-1000:]
            
            return {
                'success': True,
                'message': f'å·²å­¸ç¿’ä¸¦è¨˜æ†¶ä¸»é¡Œ: {topic}',
                'timestamp': timestamp,
                'topics_learned': list(self.memory_cache.keys())
            }
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def query_knowledge(self, query: str, category: Optional[str] = None) -> Dict[str, Any]:
        """
        æŸ¥è©¢çŸ¥è­˜åº«
        
        Args:
            query: æŸ¥è©¢é—œéµè©žæˆ–å•é¡Œ
            category: å¯é¸çš„é¡žåˆ¥é™åˆ¶ï¼ˆå¦‚ 'demographics', 'housing', 'commercial' ç­‰ï¼‰
            
        Returns:
            dict: æŸ¥è©¢çµæžœ
        """
        try:
            results = []
            query_lower = query.lower()
            
            # å¦‚æžœæŒ‡å®šäº†é¡žåˆ¥ï¼Œåªåœ¨è©²é¡žåˆ¥ä¸­æœç´¢
            search_data = {}
            if category and category in self.knowledge_data:
                search_data[category] = self.knowledge_data[category]
            else:
                search_data = self.knowledge_data
            
            # éžæ­¸æœç´¢åŒ¹é…çš„æ•¸æ“š
            matches = self._search_recursive(search_data, query_lower)
            
            # æ›´æ–°è¨ªå•è¨ˆæ•¸
            for match in matches:
                if 'topic' in match and match['topic'] in self.memory_cache:
                    for memory in self.memory_cache[match['topic']]:
                        memory['access_count'] += 1
            
            return {
                'success': True,
                'query': query,
                'category': category,
                'matches_found': len(matches),
                'results': matches[:20],  # é™åˆ¶è¿”å›žå‰ 20 å€‹çµæžœ
                'total_knowledge_topics': len(self.knowledge_data)
            }
        except Exception as e:
            return {'success': False, 'error': str(e)}

    def query_index(self, query: str, limit: int = 20) -> Dict[str, Any]:
        """
        ä½¿ç”¨ç´¢å¼•æª”é€²è¡Œå¿«é€Ÿæª¢ç´¢ï¼ˆè‹¥ç´¢å¼•ä¸å­˜åœ¨å‰‡å›žé€€åˆ°éžæ­¸æœç´¢ï¼‰ã€‚

        Returns:
            dict: æª¢ç´¢çµæžœï¼ˆitems ç‚ºç´¢å¼•æ¢ç›®ï¼ŒåŒ…å« path/snippet/keywords ç­‰ï¼‰
        """
        try:
            q = (query or '').strip()
            if not q:
                return {'success': False, 'error': 'query is required'}

            # If no index loaded, fallback
            if not self.index_data or 'items' not in self.index_data:
                fallback = self.query_knowledge(q)
                return {
                    'success': True,
                    'query': q,
                    'mode': 'fallback_recursive',
                    'matches_found': fallback.get('matches_found', 0),
                    'items': fallback.get('results', [])[:limit],
                }

            items = self.index_data.get('items', [])
            inv = self.index_data.get('inverted_index', {})
            tokenized = self._tokenize(q)
            candidate_ids = set()
            for t in tokenized:
                for item_id in inv.get(t, []):
                    candidate_ids.add(item_id)

            # If no token matches, do substring scan over snippets (small KB)
            if not candidate_ids:
                q_lower = q.lower()
                scored = []
                for it in items:
                    snippet = str(it.get('snippet', ''))
                    title = str(it.get('title', ''))
                    hay = (title + ' ' + snippet).lower()
                    if q_lower in hay:
                        scored.append((3, it))
                scored.sort(key=lambda x: x[0], reverse=True)
                out = [it for _, it in scored[:limit]]
                return {
                    'success': True,
                    'query': q,
                    'mode': 'index_substring_scan',
                    'matches_found': len(out),
                    'items': out
                }

            # Score candidates by token overlap + substring match
            scored = []
            q_lower = q.lower()
            by_id = {it.get('id'): it for it in items}
            for cid in candidate_ids:
                it = by_id.get(cid)
                if not it:
                    continue
                kw = set(it.get('keywords', []))
                base = len(kw.intersection(set(tokenized)))
                text = (str(it.get('title', '')) + ' ' + str(it.get('snippet', ''))).lower()
                if q_lower in text:
                    base += 2
                scored.append((base, it))

            scored.sort(key=lambda x: x[0], reverse=True)
            out = [it for _, it in scored[:limit]]
            return {
                'success': True,
                'query': q,
                'mode': 'index',
                'matches_found': len(out),
                'items': out
            }
        except Exception as e:
            return {'success': False, 'error': str(e)}

    def _tokenize(self, text: str) -> List[str]:
        """
        æ··åˆä¸­è‹±æ•¸ tokenizationï¼š
        - è‹±æ•¸ï¼šä¾å–®å­—åˆ‡åˆ†
        - ä¸­æ–‡ï¼šæŠ“é€£çºŒä¸­æ–‡ç‰‡æ®µï¼ˆ2~12 å­—ï¼‰ä½œç‚º tokenï¼Œä¸¦è£œ 2~4 å­— n-grams ä»¥åˆ©æŸ¥è©¢ï¼ˆä¾‹å¦‚ã€Œè€åŒ– æŒ‡æ•¸ã€å¯å‘½ä¸­ã€Œè€åŒ–æŒ‡æ•¸é«˜é”...ã€ï¼‰
        """
        s = (text or '').strip().lower()
        if not s:
            return []
        tokens: List[str] = []
        # English/number tokens
        tokens.extend([t for t in re.split(r'[^a-z0-9_]+', s) if len(t) >= 2])
        # Chinese segments
        for seg in re.findall(r'[\u4e00-\u9fff]{2,12}', text):
            tokens.append(seg)
            # add 2~4 char ngrams
            for n in (2, 3, 4):
                if len(seg) >= n:
                    for i in range(0, len(seg) - n + 1):
                        tokens.append(seg[i:i+n])
        # Dedup while preserving order
        seen = set()
        out = []
        for t in tokens:
            if t not in seen:
                seen.add(t)
                out.append(t)
        return out
    
    def _search_recursive(self, data: Any, query: str, path: str = '', depth: int = 0) -> List[Dict[str, Any]]:
        """
        éžæ­¸æœç´¢æ•¸æ“š
        
        Args:
            data: è¦æœç´¢çš„æ•¸æ“š
            query: æŸ¥è©¢é—œéµè©ž
            path: ç•¶å‰è·¯å¾‘
            depth: æœç´¢æ·±åº¦
            
        Returns:
            list: åŒ¹é…çµæžœåˆ—è¡¨
        """
        matches = []
        max_depth = 10  # é™åˆ¶æœç´¢æ·±åº¦
        
        if depth > max_depth:
            return matches
        
        if isinstance(data, dict):
            for key, value in data.items():
                current_path = f"{path}.{key}" if path else key
                
                # æª¢æŸ¥éµæ˜¯å¦åŒ¹é…
                if query in key.lower():
                    matches.append({
                        'path': current_path,
                        'key': key,
                        'value': value,
                        'type': 'key_match'
                    })
                
                # éžæ­¸æœç´¢å€¼
                matches.extend(self._search_recursive(value, query, current_path, depth + 1))
        
        elif isinstance(data, list):
            for i, item in enumerate(data):
                current_path = f"{path}[{i}]"
                matches.extend(self._search_recursive(item, query, current_path, depth + 1))
        
        elif isinstance(data, (str, int, float)):
            # æª¢æŸ¥å€¼æ˜¯å¦åŒ¹é…
            if query in str(data).lower():
                matches.append({
                    'path': path,
                    'value': data,
                    'type': 'value_match'
                })
        
        return matches
    
    def get_knowledge_summary(self) -> Dict[str, Any]:
        """
        ç²å–çŸ¥è­˜åº«æ‘˜è¦
        
        Returns:
            dict: çŸ¥è­˜åº«æ‘˜è¦ä¿¡æ¯
        """
        try:
            summary = {
                'knowledge_base_version': self.knowledge_data.get('knowledge_base_version', 'unknown'),
                'last_updated': self.knowledge_data.get('last_updated', 'unknown'),
                'community_name': self.knowledge_data.get('community_name', 'unknown'),
                'total_topics': len(self.knowledge_data),
                'topics': list(self.knowledge_data.keys()),
                'learning_history_count': len(self.learning_history),
                'memory_cache_topics': list(self.memory_cache.keys()),
                'total_memories': sum(len(memories) for memories in self.memory_cache.values())
            }
            
            # æ·»åŠ é—œéµçµ±è¨ˆæ•¸æ“š
            if 'key_statistics' in self.knowledge_data:
                summary['key_statistics'] = self.knowledge_data['key_statistics']
            
            # æ·»åŠ é—œéµæ´žå¯Ÿ
            if 'critical_insights' in self.knowledge_data:
                summary['critical_insights'] = self.knowledge_data['critical_insights']
            
            return summary
        except Exception as e:
            return {'error': str(e)}
    
    def get_context_for_ai(self, query: str, max_context_length: int = 2000) -> str:
        """
        ç‚º AI ç”Ÿæˆä¸Šä¸‹æ–‡æç¤º
        
        Args:
            query: æŸ¥è©¢æˆ–å•é¡Œ
            max_context_length: æœ€å¤§ä¸Šä¸‹æ–‡é•·åº¦ï¼ˆå­—ç¬¦æ•¸ï¼‰
            
        Returns:
            str: æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        try:
            # æŸ¥è©¢ç›¸é—œçŸ¥è­˜
            query_result = self.query_knowledge(query)
            
            if not query_result.get('success') or query_result.get('matches_found', 0) == 0:
                # å¦‚æžœæ²’æœ‰æ‰¾åˆ°åŒ¹é…ï¼Œè¿”å›žçŸ¥è­˜åº«æ‘˜è¦
                summary = self.get_knowledge_summary()
                return f"çŸ¥è­˜åº«æ‘˜è¦:\n{json.dumps(summary, ensure_ascii=False, indent=2)}"
            
            # æ§‹å»ºä¸Šä¸‹æ–‡
            context_parts = []
            context_parts.append(f"# äº”å¸¸ç¤¾å€çŸ¥è­˜åº«æŸ¥è©¢çµæžœ\n")
            context_parts.append(f"æŸ¥è©¢: {query}\n")
            context_parts.append(f"æ‰¾åˆ° {query_result['matches_found']} å€‹ç›¸é—œçµæžœ\n\n")
            
            # æ·»åŠ åŒ¹é…çµæžœ
            for i, result in enumerate(query_result['results'][:10], 1):
                context_parts.append(f"## çµæžœ {i}\n")
                context_parts.append(f"è·¯å¾‘: {result.get('path', 'N/A')}\n")
                if 'key' in result:
                    context_parts.append(f"é—œéµå­—: {result['key']}\n")
                if 'value' in result:
                    value_str = str(result['value'])
                    if len(value_str) > 200:
                        value_str = value_str[:200] + "..."
                    context_parts.append(f"å…§å®¹: {value_str}\n")
                context_parts.append("\n")
            
            context = "\n".join(context_parts)
            
            # å¦‚æžœè¶…éŽæœ€å¤§é•·åº¦ï¼Œæˆªæ–·
            if len(context) > max_context_length:
                context = context[:max_context_length] + "...\n[å…§å®¹å·²æˆªæ–·]"
            
            return context
        except Exception as e:
            return f"ç”Ÿæˆä¸Šä¸‹æ–‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
    
    def _summarize_data(self, data: Any) -> str:
        """ç”Ÿæˆæ•¸æ“šæ‘˜è¦"""
        try:
            if isinstance(data, dict):
                keys = list(data.keys())[:5]
                return f"å­—å…¸åŒ…å« {len(data)} å€‹éµ: {', '.join(keys)}"
            elif isinstance(data, list):
                return f"åˆ—è¡¨åŒ…å« {len(data)} å€‹é …ç›®"
            else:
                data_str = str(data)
                return data_str[:100] + "..." if len(data_str) > 100 else data_str
        except:
            return "ç„¡æ³•ç”Ÿæˆæ‘˜è¦"
    
    def save_learning_history(self, file_path: str = 'ai_learning_history.json'):
        """ä¿å­˜å­¸ç¿’æ­·å²"""
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump({
                    'learning_history': self.learning_history,
                    'memory_cache_summary': {
                        topic: len(memories) for topic, memories in self.memory_cache.items()
                    }
                }, f, ensure_ascii=False, indent=2)
            return {'success': True, 'message': f'å­¸ç¿’æ­·å²å·²ä¿å­˜åˆ°: {file_path}'}
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def load_community_analysis(self):
        """åŠ è¼‰ä¸¦å­¸ç¿’ç¤¾å€åˆ†æžæ•¸æ“š"""
        try:
            if not self.knowledge_data:
                return {'success': False, 'error': 'çŸ¥è­˜åº«æœªåŠ è¼‰'}
            
            # å­¸ç¿’å„å€‹ä¸»é¡Œ
            topics_learned = []
            
            # åœ°ç†åˆ†æž
            if 'geographic_analysis' in self.knowledge_data:
                self.learn_and_memorize('geographic_analysis', self.knowledge_data['geographic_analysis'])
                topics_learned.append('geographic_analysis')
            
            # ä½å®…åˆ†æž
            if 'housing_analysis' in self.knowledge_data:
                self.learn_and_memorize('housing_analysis', self.knowledge_data['housing_analysis'])
                topics_learned.append('housing_analysis')
            
            # äººå£çµ±è¨ˆ
            if 'demographics' in self.knowledge_data:
                self.learn_and_memorize('demographics', self.knowledge_data['demographics'])
                topics_learned.append('demographics')
            
            # å•†æ¥­ç”Ÿæ…‹
            if 'commercial_ecosystem' in self.knowledge_data:
                self.learn_and_memorize('commercial_ecosystem', self.knowledge_data['commercial_ecosystem'])
                topics_learned.append('commercial_ecosystem')
            
            # äº¤é€šé‹è¼¸
            if 'transportation' in self.knowledge_data:
                self.learn_and_memorize('transportation', self.knowledge_data['transportation'])
                topics_learned.append('transportation')
            
            # ç¤¾æœƒä¼æ¥­è§£æ±ºæ–¹æ¡ˆ
            if 'social_enterprise_solutions' in self.knowledge_data:
                self.learn_and_memorize('social_enterprise_solutions', self.knowledge_data['social_enterprise_solutions'])
                topics_learned.append('social_enterprise_solutions')
            
            # æˆ°ç•¥å»ºè­°
            if 'strategic_recommendations' in self.knowledge_data:
                self.learn_and_memorize('strategic_recommendations', self.knowledge_data['strategic_recommendations'])
                topics_learned.append('strategic_recommendations')
            
            # ç³»çµ±æ•´åˆé»ž
            if 'system_integration_points' in self.knowledge_data:
                self.learn_and_memorize('system_integration_points', self.knowledge_data['system_integration_points'])
                topics_learned.append('system_integration_points')
            
            return {
                'success': True,
                'message': 'ç¤¾å€åˆ†æžæ•¸æ“šå·²å­¸ç¿’ä¸¦è¨˜æ†¶',
                'topics_learned': topics_learned,
                'total_topics': len(topics_learned)
            }
        except Exception as e:
            return {'success': False, 'error': str(e)}


if __name__ == '__main__':
    # æ¸¬è©¦çŸ¥è­˜åº«ç³»çµ±
    print("=" * 60)
    print("AI çŸ¥è­˜åº«ç³»çµ±æ¸¬è©¦")
    print("=" * 60)
    
    kb = AIKnowledgeBase()
    
    # åŠ è¼‰ä¸¦å­¸ç¿’ç¤¾å€åˆ†æž
    print("\nðŸ“š åŠ è¼‰ä¸¦å­¸ç¿’ç¤¾å€åˆ†æžæ•¸æ“š...")
    result = kb.load_community_analysis()
    print(f"çµæžœ: {json.dumps(result, ensure_ascii=False, indent=2)}")
    
    # ç²å–çŸ¥è­˜åº«æ‘˜è¦
    print("\nðŸ“Š çŸ¥è­˜åº«æ‘˜è¦:")
    summary = kb.get_knowledge_summary()
    print(json.dumps(summary, ensure_ascii=False, indent=2))
    
    # æ¸¬è©¦æŸ¥è©¢
    print("\nðŸ” æ¸¬è©¦æŸ¥è©¢: 'äººå£'")
    query_result = kb.query_knowledge('äººå£')
    print(f"æ‰¾åˆ° {query_result.get('matches_found', 0)} å€‹åŒ¹é…çµæžœ")
    
    # ç”Ÿæˆ AI ä¸Šä¸‹æ–‡
    print("\nðŸ¤– ç”Ÿæˆ AI ä¸Šä¸‹æ–‡:")
    context = kb.get_context_for_ai('äº”å¸¸é‡Œäººå£çµæ§‹')
    print(context[:500] + "..." if len(context) > 500 else context)
    
    print("\n" + "=" * 60)
